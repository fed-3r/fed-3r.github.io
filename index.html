<!DOCTYPE html>
<html lang="en">

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="styles.css">

<head>
    <meta charset="UTF-8">
    <title>Fed3R</title>
    <!-- <script src="lens.js"></script> -->
</head>
<body>

<br>
<div style="text-align: center;" id="top">
        <span style="font-size:26px">Accelerating Heterogeneous Federated Learning <br>
            with Closed-form Classifiers</span><br><br><br>
</div>

<header>
    <table>
        <tbody>
        <tr>
            <td width="160px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <a href="https://scholar.google.com/citations?user=rwto7AgAAAAJ&hl=en">Eros Fanì</a>
                        <sup>1</sup>
                    </span>
                </div>
            </td>
            <td width="160px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <a href="https://scholar.google.com/citations?user=vBBJ2wkAAAAJ&hl=en">
                            Raffaello Camoriano
                        </a>
                        <sup>1 2</sup>
                    </span>
                </div>
            </td>
            <td width="160px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <a href="https://scholar.google.com/citations?user=mHbdIAwAAAAJ&hl=en">
                            Barbara Caputo
                        </a>
                        <sup>1 3</sup>
                    </span>
                </div>
            <td width="160px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <a href="https://marcociccone.github.io/">
                            Marco Ciccone
                        </a>
                        <sup>1</sup>
                    </span>
                </div>
            </td>
        </tr>

        </tbody>
    </table>

    <br>

    <table>
        <tbody>
        <tr>
            <td width="50px">
                <div style="text-align: center;">
                    <span style="font-size:16px"></span>
                </div>
            </td>
            <td width="250px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <sup>1</sup>DAUIN, PoliTO, Italy
                    </span>
                </div>
            </td>
            <td width="250px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <sup>2</sup>IIT, Genoa, Italy
                    </span>
                </div>
            </td>
            <td width="250px">
                <div style="text-align: center;">
                    <span style="font-size:16px">
                        <sup>3</sup>CINI, Rome, Italy
                    </span>
                </div>
            </td>
            <td width="50px">
                <div style="text-align: center;">
                    <span style="font-size:16px"></span>
                </div>
            </td>
        </tr>
        </tbody>
    </table>
    <table>
        <tbody>
        <tr>
            <td width="200px">
                <div style="text-align: center;">
                    <br>
                    <span style="font-size:20px">
                        <a href="https://github.com/Erosinho13/Fed3R">GitHub</a>
                    </span>
                </div>
            </td>

            <td width="200px">
                <div style="text-align: center;">
                    <br>
                    <span style="font-size:20px">
                        <a href="https://arxiv.org/pdf/2406.01116">arXiv</a>
                        <br/>
                    </span>
                </div>
            </td>

        </tr>
        </tbody>
    </table>
    <br/>

    <div class="corr">Corresponding author email: <a href="mailto:eros.fani@polito.it">eros.fani@polito.it</a> </div>

</header>

<br>
<hr>

<section>
    <div class="news">
    <h3>NEWS</h3>
    <ul>
        <li> <div class="date"> 20/07/2024 </div> - We will be presenting at ICML24! Join us at the Poster Session 5,
            11:30 a.m. - 1 p.m. CEST, Hall C 4-9 #2507 Exhibition Congress Center, Wien (AU), July 25, 2024 </li>
        <li> <div class="date"> 20/07/2024 </div> -The official video presentation is online
            <a href="https://youtu.be/ZbNrbHTGDIs" target="_blank">here</a>! </li>
        <li> <div class="date"> 13/07/2024 </div> -The official poster is online
            <a href="https://icml.cc/media/PosterPDFs/ICML%202024/33597.png?t=1721396215.675697"
               target="_blank">here</a>! </li>
        <li> <div class="date"> 01/07/2024 </div> - The official code will be released after the summer! </li>
    </ul>
    </div>
</section>

<br>
<hr>



<nav style="text-align: center;">
    <a href="#abstract">Abstract</a> / <a href="#cite">Cite us</a> / <a href="#tlds">TLDR</a> /
    <a href="#method">Method</a> / <a href="#datasets">Datasets</a> / <a href="#props">Fed3R properties</a> /
    <a href="#conclusion">Conclusion</a> / <a href="#ack">Acknowledgements</a>
</nav>

<hr>

<section id="abstract">
    <div style="text-align: center;">
        <h2> Abstract </h2>
    </div>

    Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data
    distributions cause client drift and biased local solutions, particularly pronounced in the final classification
    layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce Federated Recursive
    Ridge Regression (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging
    pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the
    clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and
    efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources
    than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax
    classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our
    findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more
    discriminative features in cross-device settings.
</section>

<a href="#top" class="back-to-top">[Back to top]</a>
<br>
<hr>

<section id="cite">

    <h2> How to cite us </h2>
    <div>
        <pre>
    @article{fani2024accelerating,
        title={Accelerating Heterogeneous Federated Learning with Closed-form Classifiers},
        author={Fanì, Eros and Camoriano, Raffaello and Caputo, Barbara
                and Ciccone, Marco},
        journal={Proceedings of the International Conference on Machine Learning},
        year={2024}
    }
        </pre>
    </div>

</section>

<a href="#top" class="back-to-top">[Back to top]</a>
<br>
<hr>

<section id="tldr">

    <h2> TLDR takeaways </h2>

    <p>
        In FL, clients’ data reveal individual user habits, preferences, and locations, which challenges the traditional
        assumption in machine learning that all data points are independent and identically distributed. This is known
        in FL as Statistical Heterogeneity. During training, statistical heterogeneity causes local updates to diverge
        from the global optimum. Consequently, the overall speed of convergence can be significantly reduced.
    </p>

    <p>
        Recent works show that, in neural networks, client drift primarily affects the classifier.
        In real-world cross-device scenarios, clients have access to different classes. Suppose client
        <img src="images/k.png" alt="k" class="inline-image-noalign"/> is the only one with access to the class “dog”.
        Because of partial participation, the same client is typically not sampled in two consecutive rounds. If client
        <img src="images/k.png" alt="k" class="inline-image-noalign"/> is sampled in one round but not in the next
        round, the model can develop data recency bias, forgetting the knowledge about the class “dog”. This phenomenon
        is well-studied in  areas such as Continual Learning. In classification, it occurs because the softmax
        classifier is prone to forgetting when updated with data in a non-i.i.d. or class-imbalanced manner.
    </p>

    <p>
        Therefore, in this work, we aim to answer the following question: Is it possible to design an efficient FL
        method that is robust to client drift in heterogeneous settings and unaffected by classifier bias? Luckily, the
        answer is yes, exploiting the properties of closed-form linear classifiers.
    </p>

    <p>
        Indeed, we propose a new robust and efficient algorithm for federated learning based on Ridge Regression (RR),
        that  we named Federated Recursive Ridge Regression (<span class="fed3r">Fed3R</span>). Thanks to the linearity
        of its formulation, Fed3R is immune to statistical heterogeneity, guarantees faster convergence than the
        baselines, and severely reduces computations and communication costs.
    </p>

    <p>
        In addition, we propose two additional variants of Fed3R: Fed3R with Random Features
        (<span class="fed3rrf">Fed3R-RF</span>), a non-linear
        version of the algorithm based on random Fourier features mapping to approximate the Kernel Ridge Regression
        solution while keeping the same properties of Fed3R, and Fed3R with Fine-Tuning
        (<span class="fed3rft">Fed3R+FT</span>), which allows fine-tuning the whole model, the
        feature extractor only, or the classifier only, after initializing the model with the Fed3R classifier.
    </p>

    <a href="#top" class="back-to-top">[Back to top]</a>

</section>

<br>
<hr>

<section id="method">

    <h2> Method </h2>

    <h3 class="fed3r"> Federated Recursive Ridge Regression (Fed3R) </h3>

    <h4> Step 1 (client side) - Local computations </h4>

    <p>
        Each client asynchronously computes local statistics using its local dataset and a pre-trained feature extractor
        <img src="images/varphi.png" alt="varphi" class="inline-image"/>.
    </p>

    <img class="fed3r-steps" src="images/fed3r_step1.png" alt="Fed3R step 1">

    <h4> Step 2 (server side) - Aggregation </h4>

    <p> The server collects these statistics and aggregates them to form the matrices
        <img src="images/A.png" alt="A" class="inline-image-noalign"/> and
        <img src="images/b.png" alt="b" class="inline-image-noalign"/>, which are used to compute the
        optimal regularized least squares classifier. The aggregation guarantees an exact solution equivalent to the
        centralized solution. </p>

    <img class="fed3r-steps" src="images/fed3r_step2.png" alt="Fed3R step 2">

    <h3 class="fed3rrf"> Fed3R with Random Features (Fed3R-RF) </h3>

    <p>
        As pre-trained feature extractors may not be expressive enough to separate features for complex learning
        problems linearly, we also introduce Fed3R-RF, which first performs a nonlinear random features mapping of the
        latent feature space to a new higher-dimensional feature space by approximating the corresponding kernel feature
        map.
    </p>

    <h3 class="fed3rft"> Fed3R with Fine-Tuning (Fed3R-FT) </h3>

    <p>
        Fed3R performance relies on the quality of the pre-trained feature extractor, which is frozen. Therefore, we
        propose Fed3R+FT, where a fine-tuning stage follows the classifier initialization. First, Fed3R+FT learns a
        Fed3R classifier using a pretrained feature extractor. Then, it initializes a softmax classifier using the
        parameters of the Fed3R classifier. Finally, the whole model is fine-tuned using a traditional FL algorithm. As
        the Fed3R classifier is the optimal Regularized Least Squares classifier obtained using the pre-trained feature
        extractor, it provides a stable starting point that can mitigate client drift and destructive interference
        during aggregation.
    </p>

    <p>
        We propose three different fine-tuning strategies for Fed3R+FT:
    </p>
    <ul>
        <li><span class="bold">Fed3R + FT</span>: the whole model is fine-tuned.</li>
        <li><span class="bold">Fed3R + FTlp</span>: only the classifier is fine-tuned.</li>
        <li><span class="bold">Fed3R + FTfeat</span>: only the feature extractor is fine-tuned.</li>
    </ul>

    <a href="#top" class="back-to-top">[Back to top]</a>

</section>

<br>
<hr>

<section id="datasets">

    <h2> Datasets </h2>

    <p>
        In our experiments, we mainly consider cross-device FL scenarios with thousands of clients and high statistical
        heterogeneity.
    </p>
    <div>
        <table>
            <tr>
                <th>Dataset</th>
                <th>Split</th>
                <th>Samples per client (avg)</th>
                <th>K</th>
                <th>C</th>
            </tr>
            <tr>
                <td>Landmarks</td>
                <td>Users-160K</td>
                <td>119.9</td>
                <td>1262</td>
                <td>2028</td>
            </tr>
            <tr>
                <td>iNaturalist</td>
                <td>Users-120K</td>
                <td>13.0</td>
                <td>9275</td>
                <td>1203</td>
            </tr>
            <tr>
                <td>iNaturalist</td>
                <td>Geo-100</td>
                <td>33.4</td>
                <td>3606</td>
                <td>1203</td>
            </tr>
            <tr>
                <td>iNaturalist</td>
                <td>Geo-300</td>
                <td>99.6</td>
                <td>1208</td>
                <td>1203</td>
            </tr>
            <tr>
                <td>iNaturalist</td>
                <td>Geo-1K</td>
                <td>326.9</td>
                <td>368</td>
                <td>1203</td>
            </tr>
            <tr>
                <td>Cifar100</td>
                <td>α=0</td>
                <td>500.0</td>
                <td>100</td>
                <td>100</td>
            </tr>
        </table>
    </div>

</section>

<a href="#top" class="back-to-top">[Back to top]</a>

<br>
<hr>

<section id="props">

    <h2> Fed3R properties </h2>

    <h3>Immunity to the statistical heterogeneity</h3>

    Once all the clients have shared their statistics with the server, the matrices
    <img src="images/A.png" alt="A" class="inline-image-noalign"/> and
    <img src="images/b.png" alt="b" class="inline-image-noalign"/> are the same for all possible
    partitions of the dataset. Hence, the Fed3R solution is invariant to the particular data split across the clients;
    in other words, Fed3R is immune to statistical heterogeneity and is invariant to the clients’ sampling order.
    Indeed, Fed3R guarantees the same final solution given any FL split of the same dataset.

    <figure id="immunity_sh">
        <img class="centered-image" src="images/immunity_sh.png" alt="Fed3R is immune to statistical heterogeneity">
        <figcaption>Fed3R and Fed3R-RF invariance to different iNaturalist splits. All the curves converge to the same
            values, showing how both methods are immune to statistical heterogeneity.</figcaption>
    </figure>

    <h3>Clients are sampled only once</h3>

    Differently to most of the classical FL algorithms, each client only needs to communicate its statistics once,
    meaning it only needs to be sampled once. Therefore, the higher the participation rate, the faster Fed3R converges.

    <h3>Differences with gradient-based FL algorithms</h3>

    Unlike gradient-based FL algorithms, FED3R does not rely on common assumptions such as the smoothness of clients’
    objectives or the unbiasedness and bounded variance of stochastic gradients. In addition, Fed3R does not require
    assuming bounded gradient dissimilarity among clients, which formalizes the effect of heterogeneous local datasets.

    <h3>Fast and efficient</h3>

    <figure class="results">
        <div class="img-magnifier-container">
            <img class="centered-image" src="images/fed3r_results.png" alt="Fed3R is fast and efficient">
            <!--height="300"-->
        </div>
        <figcaption>
            Comparison between Fe3R and the baselines. From left to right: accuracy vs rounds, accuracy vs communication
            budget, accuracy vs average computation per client. Top row: Landmarks results, Bottom row: iNaturalist
            results. Fed3R shows clear advantages regarding convergence speed, communication, and computation budget
            required.
        </figcaption>
    </figure>

    <h3> Robust initialization for the softmax classifier </h3>

    <figure class="results">
        <div class="img-magnifier-container">
            <img class="centered-image" src="images/landmarks_fed3rft.png"
                 alt="Fed3R is a robust initialization for the softmax classifier">
            <!--height="300"-->
        </div>
        <figcaption>
            Comparison between Fed3R+FT and the baselines Landmarks dataset. At the convergence point of Fed3R, we
            substitute the parameters of the Fed3R classifier to the ones of the softmax and then use another algorithm
            for fine-tuning.
        </figcaption>
    </figure>

    <p>
        Fine-tuning the entire model shows benefits on Landmarks, which is more similar to cross-silo FL than
        iNaturalist. In federated settings with more clients, such as in iNaturalist, there is a significant negative
        impact during the aggregation phase for the Fed3R+FT and Fed3R+FT LP experiments, as the classifier is
        fine-tuned and becomes susceptible to the classifier bias phenomenon. Conversely, keeping the classifier fixed
        and only fine-tuning the feature extractor as in the Fed3R+FTfeat experiments prevents classifier data recency
        bias and destructive interference during the aggregation, ensuring performance improvement and clearly
        indicating that the pre-trained features were not sufficiently enough for the target task.
    </p>

    <figure class="results">
        <div class="img-magnifier-container">
            <img class="centered-image" src="images/inaturalist_fed3rft.png"
                 alt="Fed3R+FTfeat prevents destructive interference in strong heterogeneous settings">
            <!--height="300"-->
        </div>
        <figcaption>
            Comparison between Fed3R+FT in all its variants and the baselines, iNaturalist dataset.
        </figcaption>
    </figure>

</section>

<a href="#top" class="back-to-top">[Back to top]</a>
<br>
<hr>

<section id="conclusion">

    <h2>Conclusion</h2>

    <p>
        In this work, we introduce Fed3R, a family of FL algorithms based on Recursive RR.
    </p>

    <ul>
        <li> Fed3R is designed to minimize communication and computation costs and accelerate convergence speed while
             adhering to the privacy constraints of FL. </li>
        <li> Fed3R is immune to statistical heterogeneity by design and can also serve as a robust initialization for
             further fine-tuning with optimization-based FL algorithms. </li>
        <li> Our algorithm requires up to two orders of magnitude less communication and computation costs
             to convergence than the baselines and improves the accuracy up to 12% in challenging cross-device
             FL scenarios. </li>
    </ul>

    <p> Future works may extend FED3R to streaming data or personalized learning scenarios within the FL framework. </p>

</section>

<a href="#top" class="back-to-top">[Back to top]</a>

<br>
<hr>

<section id="ack">

    <div style="text-align: center;">
        <h2> Acknowledgements </h2>
    </div>

    <p>
        This study was carried out within the FAIR - Future Artificial Intelligence Research and received funding from
        the European Union Next-GenerationEU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) – MISSIONE 4 COMPONENTE 2,
        INVESTIMENTO 1.3 – D.D. 1555 11/10/2022, PE00000013). This manuscript reflects only  the authors’ views and
        opinions, neither the European Union nor the European Commission can be considered responsible for them. We
        acknowledge the CINECA award under the ISCRA initiative for the availability of high-performance computing
        resources and support. We also thank the reviewers and area chair for their valuable comments.
    </p>

</section>

<a href="#top" class="back-to-top">[Back to top]</a>

<br>
<br>
<!-- <script>
    /* Execute the magnify function: */
    magnify("myimage", 3);
    /* Specify the id of the image, and the strength of the magnifier glass: */
</script> -->

</body>
</html>